{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "<img src=\"../images/validation_comic.jpg\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Optimize Model Parameters?\n",
    "Determining the optimal model parameters is a critical aspect of hyperparameter tuning. Most machine learning models possess a set of parameters that need to be defined before the training process commences.<br>\n",
    " Take, for example, ElasticNet regression, which has both the $l1$ and $l2$ ($\\alpha$) norm penalties.\n",
    "\n",
    "One approach to parameter tuning involves haphazardly experimenting with different values, hoping to stumble upon settings that yield improved performance.<br>\n",
    "However, a smarter and more systematic method is available, known as model validation.\n",
    "\n",
    "### The Significance of Model Validation\n",
    "In the realm of supervised learning, data is typically divided into two distinct subsets:\n",
    "- **Training:** This phase allows the model to comprehend the underlying data patterns.\n",
    "- **Testing:** This stage is designed to ensure that our model generalizes well on new, unseen data.\n",
    "\n",
    "In situations where the dataset's size permits, introducing a third set can be invaluable:\n",
    "\n",
    "- **Validation:** Here, the model can explore and determine the most suitable hyperparameters while also identifying signs of overfitting.\n",
    "\n",
    "The validation set is instrumental in evaluating the performance of our trained models under various hyperparameter configurations.<br>\n",
    "Furthermore, it serves as a valuable tool for detecting and addressing overfitting, where the model excels in training data but fails to generalize to new data.<br>\n",
    "By employing this approach, we ensure that our testing set remains entirely independent from any decisions regarding our model, thereby safeguarding against potential data snooping\n",
    "\n",
    "### Cross Validation\n",
    "<div style=\"display: flex; align-items: center;\">\n",
    "  <img src=\"../images/cross_validation_diagram_1.png\" height=\"300\"/>\n",
    "  <div style=\"width: 50px;\"></div>\n",
    "  <img src=\"../images/cross_validation_diagram_2.ppm\" height=\"300\"/>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "We often have to work with smaller datasets. In these cases, we don't have the luxury of creating a new validation subset, as we need all of the training data we can get.\n",
    "\n",
    "A very common method in these cases is **cross-validation**, which takes the following steps:\n",
    "1. Split the data into \"folds\" (smaller subsets of the data)\n",
    "2. Hold out one fold as the testing set while we traing the model on the other folds\n",
    "3. Repeat this, using all folds as a testing set once\n",
    "4. Average the model's performance over all testing folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Deafault Model ------\n",
      "R^2: 0.41\n",
      "MSE: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load housing data and split it into train/test\n",
    "np.random.seed(0)\n",
    "data = fetch_california_housing()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
    "\n",
    "# Fit the models on the training data\n",
    "ols = ElasticNet()\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "print(\"------ Deafault Model ------\")\n",
    "y_pred = ols.predict(X_test)\n",
    "print(\"R^2: %.2f\" % r2_score(y_test, y_pred))\n",
    "print(\"MSE: %.2f\" % mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "Best polynomial degree: 1\n",
      "Best alpha: 0.001\n",
      "Best l1 ratio: 0.75\n",
      "\n",
      "------ Tuned Model ------\n",
      "New R^2: 0.59\n",
      "New MSE: 0.53\n"
     ]
    }
   ],
   "source": [
    "# Define search space for hyperparameters\n",
    "param_grid = {\n",
    "    'polynomialfeatures__degree': [1],\n",
    "    'elasticnet__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'elasticnet__l1_ratio': [0, 0.25, 0.5, 0.75, 1]\n",
    "}\n",
    "\n",
    "# Create model pipeline\n",
    "model = make_pipeline(\n",
    "    PolynomialFeatures(),\n",
    "    StandardScaler(),\n",
    "    ElasticNet()\n",
    ")\n",
    "\n",
    "# Grid search cross validation\n",
    "search = RandomizedSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "search.fit(X_train, y_train)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "# Find best parameters\n",
    "best_params = best_model.named_steps['elasticnet'].get_params()\n",
    "print('\\nBest polynomial degree:', search.best_estimator_.named_steps['polynomialfeatures'].degree)\n",
    "print('Best alpha:', best_params['alpha'])\n",
    "print('Best l1 ratio:', best_params['l1_ratio'])\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "print(\"\\n------ Tuned Model ------\")\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"New R^2: %.2f\" % r2_score(y_test, y_pred))\n",
    "print(\"New MSE: %.2f\" % mean_squared_error(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
